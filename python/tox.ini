[tox]
requires =
    tox>4,<5
    virtualenv>20,<21
isolated_build = True
envlist =
    ; Mirror Supported LTS DBR versions here: https://docs.databricks.com/release-notes/runtime/
    ; Use correct PySpark version based on Python version present in env name
    dbr{113,122,133,143}
    coverage-report
skip_missing_interpreters = true

[testenv]
description = run the tests under {envname}
package = wheel
wheel_build_env = .pkg
setenv =
    COVERAGE_FILE = .coverage.{envname}
basepython =    
    dbr143: py310
    dbr133: py310
    dbr122: py39
    dbr113: py39
deps =
    -rrequirements/dev.txt
    -rrequirements/{envname}.txt
commands =
    coverage erase
    coverage run -m unittest discover -s tests -p '*_tests.py'

[testenv:lint]
description = run linters
skipsdist = true
skip_install = true
deps =
    flake8
    black==24.4.1
commands =
    black --check --verbose {posargs} {toxinidir}/tempo
    flake8 --config {toxinidir}/.flake8 {toxinidir}/tempo

[testenv:type-check]
description = run type checks
skipsdist = true
skip_install = true
deps =
    mypy>=1,<2
    pandas-stubs>=2,<3
    numpy
    types-openpyxl
commands =
    mypy --install-types {toxinidir}/tempo

[testenv:build-dist]
description = build distribution
skip_install = true
deps =
    build
    semver
commands =
    python setup.py clean bdist_wheel

[testenv:build-docs]
description = build distribution
allowlist_externals = make
deps =
    -r ../docs/requirements.txt
    semver
commands =
    make --directory ../docs html

[testenv:coverage-report]
description = combine coverage data and generate reports
deps = coverage>=7,<8
skipdist = true
skip_install = true
setenv =
    COVERAGE_FILE = .coverage
commands =
    coverage combine
    coverage report -m
    coverage xml
