[tox]
requires =
    tox>4,<5
    virtualenv>20,<21
    wheel>=0.38,<1
isolated_build = true
envlist =
    format
    lint
    type-check
    build-dist
    ; Mirror Supported LTS DBR versions here: https://docs.databricks.com/release-notes/runtime/
    ; Use correct PySpark version based on Python version present in env name
    dbr{91,104,113,122,133,142}
skip_missing_interpreters = true


[testenv]
description = run the tests under {envname}
package = wheel
wheel_build_env = .pkg
setenv =
    COVERAGE_FILE = .coverage.{envname}
basepython =    
    dbr142: py310
    dbr133: py310
    dbr122: py39
    dbr113: py39
    dbr104: py38
    dbr91: py38
deps =
    -rrequirements/{envname}.txt
    -rrequirements/dev.txt
    coverage>=7,<8
commands =
    coverage --version
    coverage run -m unittest discover -s tests -p '*_tests.py'


[testenv:lint]
description = run linters
skipsdist = true
skip_install = true
deps =
    flake8
    black
commands =
    black --check {toxinidir}/tempo
    flake8 --config {toxinidir}/.flake8 {toxinidir}/tempo

[testenv:type-check]
description = run type checks
skipsdist = true
skip_install = true
deps =
    mypy>=1,<2
    pandas-stubs>=2,<3
    numpy
    types-openpyxl
commands =
    mypy --install-types {toxinidir}/tempo

[testenv:build-dist]
description = build distribution
skip_install = true
deps =
    build
commands =
    python -m build --sdist --wheel {posargs: {toxinidir}}

[testenv:cov-init]
setenv =
    COVERAGE_FILE = .coverage
commands =
    coverage erase

[testenv:coverage-report]
description = combine coverage data and generate reports
deps = coverage>=7,<8
skipdist = true
skip_install = true
setenv =
    COVERAGE_FILE = .coverage
commands =
    coverage --version
    coverage combine
    coverage report -m
    coverage xml
