[tox]
requires =
    tox>4,<5
    virtualenv>20,<21
    wheel>=0.38,<1
isolated_build = true
envlist =
    format
    lint
    type-check
    build-dist
    ; Mirror Supported LTS DBR versions here: https://docs.databricks.com/release-notes/runtime/
    ; Use correct PySpark version based on Python version present in env name
    py37-pyspark300,
    py38-pyspark{312,321},
    py39-pyspark{330,332}
skip_missing_interpreters = true


[testenv]
description = run the tests under {envname}
package = wheel
wheel_build_env = .pkg
setenv =
    COVERAGE_FILE = .coverage.{envname}
deps =
    pyspark300: pyspark==3.0.0
    pyspark312: pyspark==3.1.2
    pyspark321: pyspark==3.2.1
    pyspark330: pyspark==3.3.0
    pyspark332: pyspark==3.3.2
    coverage>=7,<8
    -rrequirements.txt
commands =
    coverage --version
    coverage run -m unittest discover -s tests -p '*_tests.py'
    coverage report -m --data-file .coverage.{envname}

[testenv:format]
description = run formatters
skipsdist = true
skip_install = true
deps =
    black
commands =
    black {toxinidir}

[testenv:lint]
description = run linters
skipsdist = true
skip_install = true
deps =
    flake8
    black
commands =
    black --check {toxinidir}
    flake8

[testenv:type-check]
description = run type checks
skipsdist = true
skip_install = true
deps =
    mypy>=1,<2
    pandas-stubs>=2,<3
    types-pytz>=2023,<2024
    -rrequirements.txt
commands =
    mypy {toxinidir}/tempo

[testenv:build-dist]
description = build distribution
skip_install = true
deps =
    build
commands =
    python -m build --sdist --wheel {posargs: {toxinidir}}

[testenv:cov-init]
setenv =
    COVERAGE_FILE = .coverage
commands =
    coverage erase

[testenv:coverage-report]
description = combine coverage data and generate reports
deps = coverage>=7,<8
skipdist = true
skip_install = true
setenv =
    COVERAGE_FILE = .coverage
commands =
    coverage --version
    coverage combine
    coverage report -m
    coverage xml
